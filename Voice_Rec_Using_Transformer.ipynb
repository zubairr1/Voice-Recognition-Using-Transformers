{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61e89d88bd18494f86e5a8501c4b3df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7187aff02a9416dab3da5f76279bf1c",
              "IPY_MODEL_3abbbb61c60449408448e3e9d2cdf933",
              "IPY_MODEL_2a9c3b6d17684314a543accb2941b4bc"
            ],
            "layout": "IPY_MODEL_2afdfb78fd594e98977d1ec65e54a691"
          }
        },
        "a7187aff02a9416dab3da5f76279bf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e64c7757444de58b1e435f31a535b9",
            "placeholder": "​",
            "style": "IPY_MODEL_6647e79989d34c9680d27bc7262914b7",
            "value": "Downloading builder script: "
          }
        },
        "3abbbb61c60449408448e3e9d2cdf933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b92eae2a2c143eb86bcbacbd6743899",
            "max": 2318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_186b7d4b31a44e0a825d10f794fa5b48",
            "value": 2318
          }
        },
        "2a9c3b6d17684314a543accb2941b4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5ec3fb947c4c0e9109fc209ed0b520",
            "placeholder": "​",
            "style": "IPY_MODEL_1304b15d93ca437d84646840f81a5dd7",
            "value": " 6.50k/? [00:00&lt;00:00, 305kB/s]"
          }
        },
        "2afdfb78fd594e98977d1ec65e54a691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4e64c7757444de58b1e435f31a535b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6647e79989d34c9680d27bc7262914b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b92eae2a2c143eb86bcbacbd6743899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186b7d4b31a44e0a825d10f794fa5b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b5ec3fb947c4c0e9109fc209ed0b520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1304b15d93ca437d84646840f81a5dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zubairr1/Voice-Recognition-Using-Transformers/blob/main/Voice_Rec_Using_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3amlYSPc4n4",
        "outputId": "d17af2e6-eae3-4464-9993-d98a82d035c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Mounted at /content/drive\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "Successfully installed accelerate-0.33.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Install pydub\n",
        "!pip install pydub\n",
        "\n",
        "# Ensure ffmpeg is available (only necessary on some platforms like Google Colab)\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "\n",
        "import os\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "!pip install audiomentations\n",
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, Shift\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_dTuHQJrCBs",
        "outputId": "87c62973-0972-4292-a863-9cc2ca54c36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.26.4)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.10.2.post1)\n",
            "Requirement already satisfied: scipy<1.13,>=1.4 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.12.0)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.4.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio_files(audio_path, target_sr=16000, max_duration=20, max_files_per_singer=20):\n",
        "    audio_data = []\n",
        "    labels = []\n",
        "\n",
        "    singer_folders = os.listdir(audio_path)\n",
        "\n",
        "    for singer in singer_folders:\n",
        "        singer_path = os.path.join(audio_path, singer)\n",
        "        audio_files = os.listdir(singer_path)\n",
        "\n",
        "        # Shuffle and limit the number of files per singer\n",
        "        random.shuffle(audio_files)\n",
        "        audio_files = audio_files[:max_files_per_singer]\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            file_path = os.path.join(singer_path, audio_file)\n",
        "\n",
        "            # Load and resample audio using torchaudio\n",
        "            waveform, sample_rate = torchaudio.load(file_path)\n",
        "            if sample_rate != target_sr:\n",
        "                waveform = torchaudio.functional.resample(waveform, sample_rate, target_sr)\n",
        "\n",
        "            # Convert to mono if stereo\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            # Trim or pad to max_duration\n",
        "            if waveform.shape[1] > max_duration * target_sr:\n",
        "                waveform = waveform[:, :max_duration * target_sr]\n",
        "            else:\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, max_duration * target_sr - waveform.shape[1]))\n",
        "\n",
        "            # Normalize\n",
        "            waveform = waveform / torch.max(torch.abs(waveform))\n",
        "\n",
        "            # Convert to NumPy array and append\n",
        "            audio_data.append(waveform.squeeze().numpy()) # Convert to NumPy array here\n",
        "            labels.append(singer)\n",
        "\n",
        "    # Stack the NumPy arrays into a single array\n",
        "    return np.stack(audio_data), np.array(labels) # Stack the NumPy arrays"
      ],
      "metadata": {
        "id": "2f1Jvp31rMCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install audiomentations\n",
        "\n",
        "import audiomentations\n",
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch\n",
        "\n",
        "def augment_audio(waveform, sr):\n",
        "    augment = Compose([\n",
        "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
        "        # Use TimeStretch instead of Shift, and adjust parameters accordingly\n",
        "        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
        "    ])\n",
        "    augmented = augment(samples=waveform, sample_rate=sr)\n",
        "    return augmented"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6FlZCn7rNVf",
        "outputId": "e06d2b77-542f-4793-c21b-2678a02327c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.26.4)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.10.2.post1)\n",
            "Requirement already satisfied: scipy<1.13,>=1.4 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.12.0)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.4.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "audio_path = '/content/drive/MyDrive/DATASET/'\n",
        "audio_data, labels = load_audio_files(audio_path, max_duration=20, max_files_per_singer=20)\n",
        "\n",
        "unique_labels = np.unique(labels)\n",
        "print(\"Unique labels (singer names):\", unique_labels)\n",
        "\n",
        "# Create a mapping of labels to integers\n",
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "\n",
        "# Convert string labels to integer ids\n",
        "label_ids = np.array([label_to_id[label] for label in labels])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    audio_data, label_ids, test_size=0.2, stratify=label_ids, random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGt0N_MWrWlS",
        "outputId": "5df2c554-3644-49b6-f66d-2efc949f6cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels (singer names): ['SG' 'SPB' 'SRM']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets with augmentation\n",
        "def create_dataset_with_augmentation(data, labels):\n",
        "    augmented_data = []\n",
        "    augmented_labels = []\n",
        "    for waveform, label in zip(data, labels):\n",
        "        augmented_data.append(waveform)\n",
        "        augmented_labels.append(label)\n",
        "        augmented_waveform = augment_audio(waveform, 16000)\n",
        "        augmented_data.append(augmented_waveform)\n",
        "        augmented_labels.append(label)\n",
        "    return Dataset.from_dict({\"input_values\": augmented_data, \"label\": augmented_labels})\n"
      ],
      "metadata": {
        "id": "q92DbZ_1raCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "def create_dataset_with_augmentation(train_data, train_labels, sample_rate=16000):\n",
        "    augmented_data = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for audio, label in zip(train_data, train_labels):\n",
        "        # Convert audio to float32 if it's not already\n",
        "        audio_float = audio.astype(np.float32)\n",
        "\n",
        "        # Normalize the audio to the range [-1, 1]\n",
        "        audio_normalized = audio_float / np.max(np.abs(audio_float))\n",
        "\n",
        "        # Apply augmentation using the augment_audio function\n",
        "        augmented_audio = augment_audio(audio_normalized, sample_rate)\n",
        "\n",
        "        # Add both original and augmented audio to the dataset\n",
        "        augmented_data.append(audio_normalized)\n",
        "        augmented_labels.append(label)\n",
        "        augmented_data.append(augmented_audio)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "    return Dataset.from_dict({\"input_values\": augmented_data, \"label\": augmented_labels})\n",
        "\n",
        "# Use the modified function\n",
        "train_dataset = create_dataset_with_augmentation(train_data, train_labels)\n",
        "\n",
        "# The validation dataset creation remains the same\n",
        "val_dataset = Dataset.from_dict({\"input_values\": val_data.tolist(), \"label\": val_labels.tolist()})\n",
        "\n",
        "# Initialize feature extractor and model for audio classification\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "classification_model = AutoModelForAudioClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=len(unique_labels),\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se9s65FHrgFf",
        "outputId": "cb96295e-5698-4715-f01e-9f7bf6a0daca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset_with_augmentation(train_data, train_labels)\n",
        "val_dataset = Dataset.from_dict({\"input_values\": val_data.tolist(), \"label\": val_labels.tolist()})\n",
        "\n",
        "# Initialize feature extractor and model for audio classification\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "classification_model = AutoModelForAudioClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=len(unique_labels),\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZvsZGBProtm",
        "outputId": "a6559551-d60a-4cee-ec05-a896f165f995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(features):\n",
        "    input_features = [feature[\"input_values\"] for feature in features]\n",
        "    label_ids = [feature[\"label\"] for feature in features]\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        input_features,\n",
        "        sampling_rate=16000,\n",
        "        padding=\"max_length\",\n",
        "        max_length=int(16000 * 20),  # 20 seconds max length\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs[\"labels\"] = torch.tensor(label_ids)\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "RhZ2XNC6rsCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    labels = eval_pred.label_ids\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1\": load_metric(\"f1\").compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "ic2C8cs-rtVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    max_grad_norm=0.5,\n",
        "    gradient_checkpointing=True,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuK0cmn3r2ep",
        "outputId": "15eda048-bda0-4ac0-aac2-da8ed07e5065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=feature_extractor,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692,
          "referenced_widgets": [
            "61e89d88bd18494f86e5a8501c4b3df5",
            "a7187aff02a9416dab3da5f76279bf1c",
            "3abbbb61c60449408448e3e9d2cdf933",
            "2a9c3b6d17684314a543accb2941b4bc",
            "2afdfb78fd594e98977d1ec65e54a691",
            "a4e64c7757444de58b1e435f31a535b9",
            "6647e79989d34c9680d27bc7262914b7",
            "5b92eae2a2c143eb86bcbacbd6743899",
            "186b7d4b31a44e0a825d10f794fa5b48",
            "3b5ec3fb947c4c0e9109fc209ed0b520",
            "1304b15d93ca437d84646840f81a5dd7"
          ]
        },
        "id": "gyNP-Nj1r6eN",
        "outputId": "03879d1f-8ae7-45ea-915a-66a0ab07dc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='186' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [186/300 23:53 < 14:47, 0.13 it/s, Epoch 30.83/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.914062</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.822222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-f89cb76feb23>:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  \"f1\": load_metric(\"f1\").compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61e89d88bd18494f86e5a8501c4b3df5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 31:52, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.914062</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.822222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.250417</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.051832</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=300, training_loss=0.6019712320963542, metrics={'train_runtime': 1923.4577, 'train_samples_per_second': 2.496, 'train_steps_per_second': 0.156, 'total_flos': 8.71551101952e+17, 'train_loss': 0.6019712320963542, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multiple_samples(model, feature_extractor, audio_path, id_to_label, num_samples=100, max_duration=20):\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    singers = os.listdir(audio_path)\n",
        "    samples_per_singer = num_samples // len(singers)\n",
        "\n",
        "    for singer in singers:\n",
        "        singer_path = os.path.join(audio_path, singer)\n",
        "        audio_files = os.listdir(singer_path)\n",
        "\n",
        "        for _ in range(samples_per_singer):\n",
        "            audio_file = random.choice(audio_files)\n",
        "            file_path = os.path.join(singer_path, audio_file)\n",
        "\n",
        "            waveform, sample_rate = torchaudio.load(file_path)\n",
        "            if sample_rate != 16000:\n",
        "                waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            # Trim or pad to max_duration\n",
        "            if waveform.shape[1] > max_duration * 16000:\n",
        "                waveform = waveform[:, :max_duration * 16000]\n",
        "            else:\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, max_duration * 16000 - waveform.shape[1]))\n",
        "\n",
        "            waveform = waveform.squeeze().numpy()\n",
        "\n",
        "            inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=\"max_length\", max_length=int(16000 * max_duration), truncation=True)\n",
        "            inputs = {k: v.to('cuda').half() for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(**inputs).logits\n",
        "            predicted_class_id = logits.argmax().item()\n",
        "            predicted_label = id_to_label[predicted_class_id]\n",
        "\n",
        "            predictions.append(predicted_label)\n",
        "            true_labels.append(singer)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return predictions, true_labels, accuracy"
      ],
      "metadata": {
        "id": "i2BQevpesBqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    def __init__(self, log_path):\n",
        "        self.log_path = log_path\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        _ = logs.pop(\"total_flos\", None)\n",
        "        if state.is_local_process_zero:\n",
        "            with open(self.log_path, 'a') as f:\n",
        "                f.write(json.dumps(logs) + '\\n')"
      ],
      "metadata": {
        "id": "ZO5aHl39sCuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Create the callback\n",
        "logging_callback = LoggingCallback(\"training_log.jsonl\")\n",
        "\n",
        "# Add the callback to the trainer\n",
        "trainer.add_callback(logging_callback)\n",
        "\n",
        "# Train the model\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Print training and validation losses\n",
        "with open(\"training_log.jsonl\", 'r') as f:\n",
        "    logs = [json.loads(line) for line in f]\n",
        "\n",
        "train_losses = [log['loss'] for log in logs if 'loss' in log]\n",
        "eval_losses = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
        "\n",
        "print(\"Training Losses:\", train_losses)\n",
        "print(\"Validation Losses:\", eval_losses)\n",
        "\n",
        "# Plot the losses\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model(\"./trained_audio_classifier\")\n",
        "\n",
        "# Move model to GPU and convert to half precision\n",
        "classification_model = classification_model.to('cuda').half()\n",
        "\n",
        "# Run multi-sample test\n",
        "predictions, true_labels, accuracy = test_multiple_samples(classification_model, feature_extractor, audio_path, id_to_label, num_samples=100, max_duration=20)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(\"Prediction distribution:\", Counter(predictions))\n",
        "print(\"True label distribution:\", Counter(true_labels))\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "8Huxd7MasIcG",
        "outputId": "7694e34e-0821-4498-c2a0-0a4db02e66e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are adding a <class '__main__.LoggingCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
            ":DefaultFlowCallback\n",
            "TensorBoardCallback\n",
            "NotebookProgressCallback\n",
            "LoggingCallback\n",
            "LoggingCallback\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 20:38, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.116491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.055099</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.022189</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Losses: []\n",
            "Validation Losses: [0.1676432341337204, 0.0795186385512352, 0.028695425018668175, 0.1164906844496727, 0.1164906844496727, 0.1164906844496727, 0.0550994873046875, 0.0550994873046875, 0.0550994873046875, 0.0221888218075037, 0.0221888218075037, 0.0221888218075037]\n",
            "Test Accuracy: 0.9797979797979798\n",
            "Prediction distribution: Counter({'SRM': 35, 'SG': 33, 'SPB': 31})\n",
            "True label distribution: Counter({'SPB': 33, 'SRM': 33, 'SG': 33})\n",
            "Accuracy: 0.9797979797979798\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          SG       1.00      1.00      1.00        33\n",
            "         SPB       1.00      0.94      0.97        33\n",
            "         SRM       0.94      1.00      0.97        33\n",
            "\n",
            "    accuracy                           0.98        99\n",
            "   macro avg       0.98      0.98      0.98        99\n",
            "weighted avg       0.98      0.98      0.98        99\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "# Test the model on a single sample\n",
        "def test_single_sample(model, feature_extractor, audio_path, id_to_label, unique_labels):\n",
        "    # Randomly select a singer and a song\n",
        "    test_singer = random.choice(unique_labels)\n",
        "    test_singer_path = os.path.join(audio_path, test_singer)\n",
        "    test_audio_file = random.choice(os.listdir(test_singer_path))\n",
        "    test_audio_path = os.path.join(test_singer_path, test_audio_file)\n",
        "\n",
        "    print(f\"\\nTesting on a single sample:\")\n",
        "    print(f\"Selected singer: {test_singer}\")\n",
        "    print(f\"Selected audio file: {test_audio_file}\")\n",
        "\n",
        "    # Load and preprocess the audio\n",
        "    waveform, sample_rate = torchaudio.load(test_audio_path)\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Trim or pad to 20 seconds\n",
        "    if waveform.shape[1] > 20 * 16000:\n",
        "        waveform = waveform[:, :20 * 16000]\n",
        "    else:\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, 20 * 16000 - waveform.shape[1]))\n",
        "\n",
        "    waveform = waveform.squeeze().numpy()\n",
        "\n",
        "    # Prepare input for the model\n",
        "    test_input = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=\"max_length\", max_length=int(16000 * 20), truncation=True)\n",
        "    test_input = {k: v.to('cuda').half() for k, v in test_input.items()}  # Move input to GPU and convert to half precision\n",
        "\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(**test_input)\n",
        "        logits = output.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get predicted class and probability\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    predicted_label = id_to_label[predicted_class_id]\n",
        "    predicted_probability = probabilities[0][predicted_class_id].item()\n",
        "\n",
        "    print(f\"\\nPredicted singer: {predicted_label}\")\n",
        "    print(f\"Confidence: {predicted_probability:.2%}\")\n",
        "    print(f\"Actual singer: {test_singer}\")\n",
        "    print(\"\\nTop 3 predictions:\")\n",
        "\n",
        "    # Get top 3 predictions\n",
        "    top3_prob, top3_indices = torch.topk(probabilities, 3)\n",
        "    for i in range(3):\n",
        "        print(f\"{id_to_label[top3_indices[0][i].item()]}: {top3_prob[0][i].item():.2%}\")\n",
        "\n",
        "    return predicted_label, test_singer\n",
        "\n",
        "# Add this to your main code after training and before the multi-sample test\n",
        "print(\"\\n--- Single Sample Test ---\")\n",
        "predicted_label, actual_label = test_single_sample(classification_model, feature_extractor, audio_path, id_to_label, unique_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIgZjoKzsIXg",
        "outputId": "a3a7cb52-677a-4966-cb46-d25ddc7934da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Single Sample Test ---\n",
            "\n",
            "Testing on a single sample:\n",
            "Selected singer: SG\n",
            "Selected audio file: CLIP-8.mp3\n",
            "\n",
            "Predicted singer: SG\n",
            "Confidence: 90.68%\n",
            "Actual singer: SG\n",
            "\n",
            "Top 3 predictions:\n",
            "SG: 90.68%\n",
            "SRM: 4.72%\n",
            "SPB: 4.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import json\n",
        "\n",
        "# Save the label mapping\n",
        "label_map_path = os.path.join(\"./trained_audio_classifier\", \"label_map.json\")\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    json.dump(id_to_label, f)"
      ],
      "metadata": {
        "id": "OEWgpVq5BHIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load your trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Load the label mapping\n",
        "label_map_path = os.path.join(model_name, \"label_map.json\")\n",
        "with open(label_map_path, \"r\") as f:\n",
        "    id_to_label = json.load(f)\n",
        "\n",
        "# Function to preprocess and predict\n",
        "def predict_singer(audio_file_path):\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Resample if necessary (assuming your model expects 16kHz)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Preprocess the audio\n",
        "    inputs = feature_extractor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\")\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    predicted_singer = id_to_label[str(predicted_class_id)]\n",
        "\n",
        "    return predicted_singer\n",
        "\n",
        "# Directory containing the input MP3 files\n",
        "input_directory = \"/content/drive/MyDrive/INPUT\"\n",
        "\n",
        "# Get all MP3 files in the input directory\n",
        "mp3_files = [f for f in os.listdir(input_directory) if f.endswith('.mp3')]\n",
        "\n",
        "# Process each MP3 file\n",
        "for mp3_file in mp3_files:\n",
        "    audio_file_path = os.path.join(input_directory, mp3_file)\n",
        "    try:\n",
        "        predicted_singer = predict_singer(audio_file_path)\n",
        "        print(f\"The predicted singer for {mp3_file} is: {predicted_singer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mp3_file}: {str(e)}\")\n",
        "\n",
        "print(\"Classification complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeLSX0l5BIS-",
        "outputId": "5d6c11cc-bbfc-49e3-ec03-839056d3ad35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted singer for SG.mp3 is: SG\n",
            "The predicted singer for SPB.mp3 is: SG\n",
            "The predicted singer for SM.mp3 is: SRM\n",
            "Classification complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load your trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Load the label mapping\n",
        "label_map_path = os.path.join(model_name, \"label_map.json\")\n",
        "with open(label_map_path, \"r\") as f:\n",
        "    id_to_label = json.load(f)\n",
        "\n",
        "# Function to preprocess and predict\n",
        "def predict_singer(audio_file_path):\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Resample if necessary (assuming your model expects 16kHz)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Preprocess the audio\n",
        "    inputs = feature_extractor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\")\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    predicted_singer = id_to_label[str(predicted_class_id)]\n",
        "\n",
        "    return predicted_singer\n",
        "\n",
        "# Directory containing the input MP3 files\n",
        "input_directory = \"/content/drive/MyDrive/INPUT\"\n",
        "\n",
        "# Get all MP3 files in the input directory\n",
        "mp3_files = [f for f in os.listdir(input_directory) if f.endswith('.mp3')]\n",
        "\n",
        "# Ensure we have exactly 9 MP3 files\n",
        "if len(mp3_files) != 9:\n",
        "    print(f\"Warning: Expected 9 MP3 files, but found {len(mp3_files)} files.\")\n",
        "\n",
        "# Process each MP3 file\n",
        "for mp3_file in mp3_files:\n",
        "    audio_file_path = os.path.join(input_directory, mp3_file)\n",
        "    try:\n",
        "        predicted_singer = predict_singer(audio_file_path)\n",
        "        print(f\"The predicted singer for {mp3_file} is: {predicted_singer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mp3_file}: {str(e)}\")\n",
        "\n",
        "print(\"Classification complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmFu0ItMB7Dt",
        "outputId": "427d4990-a7d3-4919-eba7-3286f2236696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Expected 9 MP3 files, but found 3 files.\n",
            "The predicted singer for SG.mp3 is: SG\n",
            "The predicted singer for SPB.mp3 is: SG\n",
            "The predicted singer for SM.mp3 is: SRM\n",
            "Classification complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "\n",
        "# Load your trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function to preprocess and predict\n",
        "def predict_singer(audio_file_path):\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Resample if necessary (assuming your model expects 16kHz)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Preprocess the audio\n",
        "    inputs = feature_extractor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\")\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "# Map the class ID to singer name (you'll need to create this mapping)\n",
        "    singer_names = {0:\"SP BALASUBRAMANYUM\", 1:\"SHANKAR MAHADEVAN\", 2:\"SHREYA GHOSHAL\"}  # Replace with your actual singer names\n",
        "    predicted_singer = singer_names[predicted_class_id]\n",
        "\n",
        "    return predicted_singer\n",
        "\n",
        "# Use the function\n",
        "audio_file_path = \"/content/drive/MyDrive/INPUT/sg3.mp3\"\n",
        "predicted_singer = predict_singer(audio_file_path)\n",
        "print(f\"The predicted singer is: {predicted_singer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2u8DNrMfCQre",
        "outputId": "f82c17d0-954c-455c-9b36-b9a476779e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"/content/drive/MyDrive/INPUT/sg3.mp3\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7b99abecf897 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7b99abe7fb25 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42334 (0x7b99aaacb334 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7b99aaacdd34 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3aa4e (0x7b98e2c83a4e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32617 (0x7b98e2c7b617 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15ac9e (0x59ae9ffb6c9e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x59ae9ffad3cb in /usr/bin/python3)\nframe #8: <unknown function> + 0x169540 (0x59ae9ffc5540 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165c87 (0x59ae9ffc1c87 in /usr/bin/python3)\nframe #10: <unknown function> + 0x15177b (0x59ae9ffad77b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf6cb (0x7b99cc0296cb in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x59ae9ffad3cb in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6e5b (0x59ae9ffa5fab in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x59ae9ffac564 in /usr/bin/python3)\nframe #15: <unknown function> + 0x165664 (0x59ae9ffc1664 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x59ae9ffad36c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6e5b (0x59ae9ffa5fab in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x644a (0x59ae9ffa559a in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x644a (0x59ae9ffa559a in /usr/bin/python3)\nframe #24: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #26: <unknown function> + 0x13ff96 (0x59ae9ff9bf96 in /usr/bin/python3)\nframe #27: PyEval_EvalCode + 0x86 (0x59aea0091c66 in /usr/bin/python3)\nframe #28: <unknown function> + 0x23b81d (0x59aea009781d in /usr/bin/python3)\nframe #29: <unknown function> + 0x15b7f9 (0x59ae9ffb77f9 in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #31: <unknown function> + 0x178890 (0x59ae9ffd4890 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x286f (0x59ae9ffa19bf in /usr/bin/python3)\nframe #33: <unknown function> + 0x178890 (0x59ae9ffd4890 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x286f (0x59ae9ffa19bf in /usr/bin/python3)\nframe #35: <unknown function> + 0x178890 (0x59ae9ffd4890 in /usr/bin/python3)\nframe #36: <unknown function> + 0x25619f (0x59aea00b219f in /usr/bin/python3)\nframe #37: <unknown function> + 0x166eca (0x59ae9ffc2eca in /usr/bin/python3)\nframe #38: _PyEval_EvalFrameDefault + 0x81e (0x59ae9ff9f96e in /usr/bin/python3)\nframe #39: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x81e (0x59ae9ff9f96e in /usr/bin/python3)\nframe #43: <unknown function> + 0x169111 (0x59ae9ffc5111 in /usr/bin/python3)\nframe #44: PyObject_Call + 0x122 (0x59ae9ffc5db2 in /usr/bin/python3)\nframe #45: _PyEval_EvalFrameDefault + 0x294d (0x59ae9ffa1a9d in /usr/bin/python3)\nframe #46: <unknown function> + 0x169111 (0x59ae9ffc5111 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x1a27 (0x59ae9ffa0b77 in /usr/bin/python3)\nframe #48: <unknown function> + 0x2015e5 (0x59aea005d5e5 in /usr/bin/python3)\nframe #49: <unknown function> + 0x15b7f9 (0x59ae9ffb77f9 in /usr/bin/python3)\nframe #50: <unknown function> + 0x2375b5 (0x59aea00935b5 in /usr/bin/python3)\nframe #51: <unknown function> + 0x2b4142 (0x59aea0110142 in /usr/bin/python3)\nframe #52: <unknown function> + 0x14e2eb (0x59ae9ffaa2eb in /usr/bin/python3)\nframe #53: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #54: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x81e (0x59ae9ff9f96e in /usr/bin/python3)\nframe #56: <unknown function> + 0x2015e5 (0x59aea005d5e5 in /usr/bin/python3)\nframe #57: <unknown function> + 0x15b7f9 (0x59ae9ffb77f9 in /usr/bin/python3)\nframe #58: <unknown function> + 0x2375b5 (0x59aea00935b5 in /usr/bin/python3)\nframe #59: <unknown function> + 0x2b4142 (0x59aea0110142 in /usr/bin/python3)\nframe #60: <unknown function> + 0x14e2eb (0x59ae9ffaa2eb in /usr/bin/python3)\nframe #61: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #62: <unknown function> + 0x169111 (0x59ae9ffc5111 in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-01195c79e839>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Use the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0maudio_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/INPUT/sg3.mp3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mpredicted_singer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_singer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The predicted singer is: {predicted_singer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-01195c79e839>\u001b[0m in \u001b[0;36mpredict_singer\u001b[0;34m(audio_file_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_singer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Load the audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Resample if necessary (assuming your model expects 16kHz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"/content/drive/MyDrive/INPUT/sg3.mp3\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7b99abecf897 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7b99abe7fb25 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42334 (0x7b99aaacb334 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7b99aaacdd34 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3aa4e (0x7b98e2c83a4e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32617 (0x7b98e2c7b617 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15ac9e (0x59ae9ffb6c9e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x59ae9ffad3cb in /usr/bin/python3)\nframe #8: <unknown function> + 0x169540 (0x59ae9ffc5540 in /usr/bin/python3)\nframe #9: <unknown function> + 0x165c87 (0x59ae9ffc1c87 in /usr/bin/python3)\nframe #10: <unknown function> + 0x15177b (0x59ae9ffad77b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf6cb (0x7b99cc0296cb in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x59ae9ffad3cb in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6e5b (0x59ae9ffa5fab in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x59ae9ffac564 in /usr/bin/python3)\nframe #15: <unknown function> + 0x165664 (0x59ae9ffc1664 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x59ae9ffad36c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6e5b (0x59ae9ffa5fab in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x644a (0x59ae9ffa559a in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x644a (0x59ae9ffa559a in /usr/bin/python3)\nframe #24: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #26: <unknown function> + 0x13ff96 (0x59ae9ff9bf96 in /usr/bin/python3)\nframe #27: PyEval_EvalCode + 0x86 (0x59aea0091c66 in /usr/bin/python3)\nframe #28: <unknown function> + 0x23b81d (0x59aea009781d in /usr/bin/python3)\nframe #29: <unknown function> + 0x15b7f9 (0x59ae9ffb77f9 in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #31: <unknown function> + 0x178890 (0x59ae9ffd4890 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x286f (0x59ae9ffa19bf in /usr/bin/python3)\nframe #33: <unknown function> + 0x178890 (0x59ae9ffd4890 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x286f (0x59ae9ffa19bf in /usr/bin/python3)\nframe #35: <unknown function> + 0x178890 (0x59ae9ffd4890 in /usr/bin/python3)\nframe #36: <unknown function> + 0x25619f (0x59aea00b219f in /usr/bin/python3)\nframe #37: <unknown function> + 0x166eca (0x59ae9ffc2eca in /usr/bin/python3)\nframe #38: _PyEval_EvalFrameDefault + 0x81e (0x59ae9ff9f96e in /usr/bin/python3)\nframe #39: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x81e (0x59ae9ff9f96e in /usr/bin/python3)\nframe #43: <unknown function> + 0x169111 (0x59ae9ffc5111 in /usr/bin/python3)\nframe #44: PyObject_Call + 0x122 (0x59ae9ffc5db2 in /usr/bin/python3)\nframe #45: _PyEval_EvalFrameDefault + 0x294d (0x59ae9ffa1a9d in /usr/bin/python3)\nframe #46: <unknown function> + 0x169111 (0x59ae9ffc5111 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x1a27 (0x59ae9ffa0b77 in /usr/bin/python3)\nframe #48: <unknown function> + 0x2015e5 (0x59aea005d5e5 in /usr/bin/python3)\nframe #49: <unknown function> + 0x15b7f9 (0x59ae9ffb77f9 in /usr/bin/python3)\nframe #50: <unknown function> + 0x2375b5 (0x59aea00935b5 in /usr/bin/python3)\nframe #51: <unknown function> + 0x2b4142 (0x59aea0110142 in /usr/bin/python3)\nframe #52: <unknown function> + 0x14e2eb (0x59ae9ffaa2eb in /usr/bin/python3)\nframe #53: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #54: _PyFunction_Vectorcall + 0x7c (0x59ae9ffb759c in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x81e (0x59ae9ff9f96e in /usr/bin/python3)\nframe #56: <unknown function> + 0x2015e5 (0x59aea005d5e5 in /usr/bin/python3)\nframe #57: <unknown function> + 0x15b7f9 (0x59ae9ffb77f9 in /usr/bin/python3)\nframe #58: <unknown function> + 0x2375b5 (0x59aea00935b5 in /usr/bin/python3)\nframe #59: <unknown function> + 0x2b4142 (0x59aea0110142 in /usr/bin/python3)\nframe #60: <unknown function> + 0x14e2eb (0x59ae9ffaa2eb in /usr/bin/python3)\nframe #61: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\nframe #62: <unknown function> + 0x169111 (0x59ae9ffc5111 in /usr/bin/python3)\nframe #63: _PyEval_EvalFrameDefault + 0x6d7 (0x59ae9ff9f827 in /usr/bin/python3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the trained model and feature extractor\n",
        "model_name = \"./trained_audio_classifier\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def predict_singer(audio_file_path, confidence_threshold=0.5):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "        # Resample if necessary (assuming 16kHz is required)\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Convert to mono if stereo\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Normalize\n",
        "        waveform = waveform / torch.max(torch.abs(waveform))\n",
        "\n",
        "        # Preprocess the audio\n",
        "        inputs = feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Move inputs to the same device as the model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Get the predicted class and its probability\n",
        "        predicted_class_id = logits.argmax().item()\n",
        "        predicted_prob = probs[0][predicted_class_id].item()\n",
        "\n",
        "        # Map the class ID to singer name\n",
        "        id_to_label = {0: \"SP BALASUBRAMANYUM\", 1: \"SHANKAR MAHADEVAN\", 2: \"SHREYA GHOSHAL\"}\n",
        "        predicted_singer = id_to_label[predicted_class_id]\n",
        "\n",
        "        # Print probabilities for all classes\n",
        "        print(f\"Probabilities for {os.path.basename(audio_file_path)}:\")\n",
        "        for i, prob in enumerate(probs[0]):\n",
        "            print(f\"{id_to_label[i]}: {prob.item():.4f}\")\n",
        "\n",
        "        # Check if the prediction meets the confidence threshold\n",
        "        if predicted_prob >= confidence_threshold:\n",
        "            return predicted_singer, predicted_prob\n",
        "        else:\n",
        "            return \"Uncertain\", predicted_prob\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Process all audio files in the input directory\n",
        "input_directory = \"/content/drive/MyDrive/INPUT\"\n",
        "\n",
        "# List to store valid audio files\n",
        "valid_audio_files = []\n",
        "\n",
        "# Collect valid audio files\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith((\".mp3\", \".wav\", \".ogg\")):  # Add or remove file extensions as needed\n",
        "        valid_audio_files.append(filename)\n",
        "\n",
        "# Ensure we process only 9 files\n",
        "num_files_to_process = min(9, len(valid_audio_files))\n",
        "\n",
        "print(f\"Processing {num_files_to_process} audio files:\")\n",
        "\n",
        "for i in range(num_files_to_process):\n",
        "    filename = valid_audio_files[i]\n",
        "    audio_file_path = os.path.join(input_directory, filename)\n",
        "    predicted_singer, confidence = predict_singer(audio_file_path)\n",
        "    if predicted_singer:\n",
        "        print(f\"The predicted singer for {filename} is: {predicted_singer} (Confidence: {confidence:.4f})\")\n",
        "    print()  # Add a blank line for readability\n",
        "\n",
        "print(\"\\nPrediction completed for all audio files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKP5-zYdCXJX",
        "outputId": "22d4a300-405e-4516-a2ba-962831c847c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 3 audio files:\n",
            "Probabilities for SG.mp3:\n",
            "SP BALASUBRAMANYUM: 0.9072\n",
            "SHANKAR MAHADEVAN: 0.0450\n",
            "SHREYA GHOSHAL: 0.0478\n",
            "The predicted singer for SG.mp3 is: SP BALASUBRAMANYUM (Confidence: 0.9072)\n",
            "\n",
            "Probabilities for SPB.mp3:\n",
            "SP BALASUBRAMANYUM: 0.8574\n",
            "SHANKAR MAHADEVAN: 0.0609\n",
            "SHREYA GHOSHAL: 0.0817\n",
            "The predicted singer for SPB.mp3 is: SP BALASUBRAMANYUM (Confidence: 0.8574)\n",
            "\n",
            "Probabilities for SM.mp3:\n",
            "SP BALASUBRAMANYUM: 0.0533\n",
            "SHANKAR MAHADEVAN: 0.0528\n",
            "SHREYA GHOSHAL: 0.8938\n",
            "The predicted singer for SM.mp3 is: SHREYA GHOSHAL (Confidence: 0.8938)\n",
            "\n",
            "\n",
            "Prediction completed for all audio files.\n"
          ]
        }
      ]
    }
  ]
}